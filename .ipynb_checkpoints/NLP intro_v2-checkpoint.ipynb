{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:51:14.156964Z",
     "start_time": "2020-04-18T19:51:00.100909Z"
    },
    "code_folding": [],
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook loaded successfully\n",
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "CPython 3.7.6\n",
      "IPython 7.12.0\n",
      "\n",
      "pandas 0.25.1\n",
      "numpy 1.17.2\n",
      "scipy 1.4.1\n",
      "statsmodels 0.11.1\n",
      "matplotlib 3.1.3\n",
      "seaborn 0.10.0\n",
      "spacy 2.2.3\n",
      "tqdm 4.43.0\n",
      "watermark 2.0.2\n",
      "\n",
      "compiler   : MSC v.1916 64 bit (AMD64)\n",
      "system     : Windows\n",
      "release    : 10\n",
      "machine    : AMD64\n",
      "processor  : Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\n",
      "CPU cores  : 12\n",
      "interpreter: 64bit\n",
      "\u001b[1m\n",
      "============================== Info about spaCy ==============================\u001b[0m\n",
      "\n",
      "spaCy version    2.2.3                         \n",
      "Location         C:\\Users\\alexg\\Anaconda3\\envs\\PyR_202003_pandas_0_25_3\\lib\\site-packages\\spacy\n",
      "Platform         Windows-10-10.0.18362-SP0     \n",
      "Python version   3.7.6                         \n",
      "Models                                         \n",
      "\n",
      "\u001b[1m\n",
      "===================== Info about model 'en_core_web_md' =====================\u001b[0m\n",
      "\n",
      "lang             en                            \n",
      "name             core_web_md                   \n",
      "license          MIT                           \n",
      "author           Explosion                     \n",
      "url              https://explosion.ai          \n",
      "email            contact@explosion.ai          \n",
      "description      English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities.\n",
      "sources          [{'name': 'OntoNotes 5', 'url': 'https://catalog.ldc.upenn.edu/LDC2013T19', 'license': 'commercial (licensed by Explosion)'}, {'name': 'Common Crawl'}]\n",
      "pipeline         ['tagger', 'parser', 'ner']   \n",
      "version          2.2.5                         \n",
      "spacy_version    >=2.2.2                       \n",
      "parent_package   spacy                         \n",
      "labels           {'tagger': ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``'], 'parser': ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp'], 'ner': ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']}\n",
      "vectors          {'width': 300, 'vectors': 20000, 'keys': 684830, 'name': 'en_core_web_md.vectors'}\n",
      "source           C:\\Users\\alexg\\Anaconda3\\envs\\PyR_202003_pandas_0_25_3\\lib\\site-packages\\en_core_web_md\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LOAD MODULES\n",
    "from IPython.core.display import display, HTML # for max width\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "import scattertext as st\n",
    "from IPython.display import IFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import spatial\n",
    "from scipy.spatial import distance\n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    "import spacy # software for analysing text\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_lg') # a medium english language model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LOAD FUNCTIONS\n",
    "def compareMeanings(words):\n",
    "    # calculate similarities\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit([nlp(word).vector for word in words])\n",
    "    word_vecs_2d = pca.transform([nlp(word).vector for word in words])\n",
    "    \n",
    "    # create plot \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(word_vecs_2d[:,0], word_vecs_2d[:,1])\n",
    "    # for each word and coordinate pair: draw the text on the plot\n",
    "    for word, coord in zip(words, word_vecs_2d):\n",
    "        x, y = coord\n",
    "        plt.text(x, y, word, size= 15)\n",
    "    #plt.savefig('fig.jpg', dpi=1000)\n",
    "    # show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def mathsOnMeaning(start, subtract, add):\n",
    "    x = nlp.vocab[start].vector - nlp.vocab[subtract].vector + nlp.vocab[add].vector\n",
    "    \n",
    "    similar_words = []\n",
    "    for word in nlp.vocab:\n",
    "        if not (word.has_vector & word.is_lower):\n",
    "            continue\n",
    "        if not ((word.text!=start)&(word.text!=add)&(word.text!=subtract)):\n",
    "            continue\n",
    "     \n",
    "        similarity = cosine_similarity(x, word.vector)\n",
    "        similar_words.append((word, similarity))\n",
    "        \n",
    "    similar_words = sorted(similar_words, key=lambda item: -item[1])\n",
    "    result = [w[0].text for w in similar_words[:5]][0]\n",
    "    print(f\"'{start}' minus '{subtract}' add '{add}' = {result}\")\n",
    "    \n",
    "def scattertextTag(tag):\n",
    "    def getX(doc):\n",
    "        out = [x.text for x in doc if (x.tag_ in [tag])]\n",
    "        return nlp(' '.join(out))\n",
    "    df['text_x'] = df['text_doc'].apply(getX)\n",
    "    \n",
    "    corpus = st.CorpusFromParsedDocuments(df, \n",
    "                                          parsed_col='text_x',\n",
    "                                category_col='post_type'\n",
    "                                         ).build()\n",
    "    html = st.produce_scattertext_explorer(corpus,\n",
    "             category='story',\n",
    "             category_name='Patient Criticism',\n",
    "             not_category_name='Staff Responses',\n",
    "             #characteristic_scorer=None,\n",
    "             alternative_text_field='post_body',\n",
    "             show_characteristic=False,                              \n",
    "             width_in_pixels=800)\n",
    "    return html\n",
    "\n",
    "def scattertextThemes(topics):\n",
    "    topic_feature_builder = st.FeatsFromTopicModel(topics)\n",
    "    \n",
    "    #CREATE CORPUS\n",
    "    corpus = st.CorpusFromParsedDocuments(df, \n",
    "                                          parsed_col='text_doc',\n",
    "                                          category_col='post_type',\n",
    "                                          feats_from_spacy_doc=topic_feature_builder\n",
    "                                          ).build()\n",
    "    \n",
    "    # CREATE PLOT\n",
    "    html = st.produce_scattertext_explorer(corpus,\n",
    "                                          category='story',\n",
    "                                          category_name='Patient Criticism',\n",
    "                                          not_category_name = 'Hospital Response',\n",
    "                                          width_in_pixels=600,\n",
    "                                           height_in_pixels=400,\n",
    "                                          show_characteristic=False,\n",
    "                                           use_non_text_features=True,\n",
    "                                           show_top_terms=True,\n",
    "                                          minimum_term_frequency=5,\n",
    "                                          use_full_doc=True,\n",
    "                                          p_value_colors=True,\n",
    "                                          max_snippets = 11,\n",
    "                                           pmi_threshold_coefficient=0,\n",
    "                                        topic_model_term_lists=topic_feature_builder.get_top_model_term_lists()\n",
    "                                          )\n",
    "    return html\n",
    "\n",
    "def findSentence(sentence_subject, sentence_objecet, sentence_contains):\n",
    "    print('')\n",
    "    if len(sentence_contains)>0:\n",
    "        for doc in df.loc[df['post_type']=='response', 'text_doc']:\n",
    "            for sent in doc.sents:\n",
    "                for tok in sent:\n",
    "                    if tok.text.lower() in sentence_contains:\n",
    "                        for tok in sent:\n",
    "                            if 'obj' in tok.dep_:\n",
    "                                if tok.text.lower() in sentence_object:\n",
    "                                    for tok in sent:\n",
    "                                        if 'subj' in tok.dep_:\n",
    "                                            if tok.text.lower() in sentence_subject:\n",
    "                                                print(tok.sent)\n",
    "                                                #displacy.render(tok.sent, jupyter=True, style='dep')\n",
    "                                                break\n",
    "    else:\n",
    "        for doc in df.loc[df['post_type']=='response', 'text_doc']:\n",
    "            for sent in doc.sents:\n",
    "                for tok in sent:\n",
    "                    if 'obj' in tok.dep_:\n",
    "                        if tok.text.lower() in sentence_object:\n",
    "                            for tok in sent:\n",
    "                                if 'subj' in tok.dep_:\n",
    "                                    if tok.text.lower() in sentence_subject:\n",
    "                                        print(tok.sent)\n",
    "                                        #displacy.render(tok.sent, jupyter=True, style='dep')\n",
    "                                        break\n",
    "print('Notebook loaded successfully')\n",
    "\n",
    "# set up info\n",
    "%load_ext watermark\n",
    "%watermark -m -v -p pandas,numpy,scipy,statsmodels,matplotlib,seaborn,spacy,tqdm,watermark\n",
    "\n",
    "# spacy model info\n",
    "!python -m spacy info \n",
    "#!python -m spacy info en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Jupyter-notebook\" data-toc-modified-id=\"Jupyter-notebook-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Jupyter notebook</a></span></li><li><span><a href=\"#Natural-Language-Processing-(NLP)\" data-toc-modified-id=\"Natural-Language-Processing-(NLP)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Natural Language Processing (NLP)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pre-2013:-Word-Frequency\" data-toc-modified-id=\"Pre-2013:-Word-Frequency-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Pre-2013: Word Frequency</a></span></li><li><span><a href=\"#Post-2013\" data-toc-modified-id=\"Post-2013-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Post-2013</a></span></li></ul></li><li><span><a href=\"#Word-embeddings\" data-toc-modified-id=\"Word-embeddings-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Word embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Find-words-used-in-similar-contexts\" data-toc-modified-id=\"Find-words-used-in-similar-contexts-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Find words used in similar contexts</a></span></li><li><span><a href=\"#Comparing-word-meaning?\" data-toc-modified-id=\"Comparing-word-meaning?-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Comparing word meaning?</a></span></li><li><span><a href=\"#Comparing-sentence-meaning?\" data-toc-modified-id=\"Comparing-sentence-meaning?-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Comparing sentence meaning?</a></span></li><li><span><a href=\"#Scoring-sentences-against-target-meaning\" data-toc-modified-id=\"Scoring-sentences-against-target-meaning-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Scoring sentences against target meaning</a></span></li><li><span><a href=\"#Adding-and-subtracting-meanings?\" data-toc-modified-id=\"Adding-and-subtracting-meanings?-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Adding and subtracting meanings?</a></span></li><li><span><a href=\"#'Thought-vectors'\" data-toc-modified-id=\"'Thought-vectors'-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>'Thought vectors'</a></span></li></ul></li><li><span><a href=\"#Hospital-Staff-Responding-to-Criticism\" data-toc-modified-id=\"Hospital-Staff-Responding-to-Criticism-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Hospital Staff Responding to Criticism</a></span><ul class=\"toc-item\"><li><span><a href=\"#Research-context\" data-toc-modified-id=\"Research-context-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Research context</a></span></li><li><span><a href=\"#Comparing-perspectives:-words\" data-toc-modified-id=\"Comparing-perspectives:-words-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Comparing perspectives: words</a></span></li><li><span><a href=\"#Comparing-perspectives:-themes\" data-toc-modified-id=\"Comparing-perspectives:-themes-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Comparing perspectives: themes</a></span></li><li><span><a href=\"#Which-responses-have-least-perspective-taking?\" data-toc-modified-id=\"Which-responses-have-least-perspective-taking?-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Which responses have least perspective taking?</a></span></li><li><span><a href=\"#Which-responses-have-most-psychological-distancing?\" data-toc-modified-id=\"Which-responses-have-most-psychological-distancing?-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Which responses have most psychological distancing?</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T13:53:13.634174Z",
     "start_time": "2020-04-18T13:53:13.629176Z"
    }
   },
   "source": [
    "[Jupyter notebooks](https://jupyter.org/) (and more recently Jupyter Lab) are language agnostic notebooks. They can run many languages, especially: **Ju**lia, **Pyt**hon, and **R**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T14:16:34.274913Z",
     "start_time": "2020-04-18T14:16:34.270911Z"
    }
   },
   "source": [
    "Initiated in 2014, Jupyter notebooks have rapidly become a [dominant platform for data analysis](https://www.nature.com/articles/d41586-018-07196-1). There is good reason to believe that they will eventually [replace the scientific article](https://www.theatlantic.com/science/archive/2018/04/the-scientific-paper-is-obsolete/556676/) as we know it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scientific articles are texts that talk about data and analysis - but they don't allow readers to interact with the data or the analysis. Notebooks, are a step-change in scientific communication because they combine text, data, and analysis in one easy to read and reproducible package that is easy to share and interact with ([gallery of interesting notebooks](https://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "NLP refers to using computers to search, manipulate and respond to human language\n",
    "\n",
    "It is closely connected to AI, Machine Learning, Big Data\n",
    "\n",
    "The driving motive is that Google, Facebook, etc. are investing vast resources in making computers understand text, image and sound in a way that matters for humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Pre-2013: Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "To-date most psychological research using computers to analyse text has focused on word-frequency. Because the words we speak are closely connected to our thought patterns, this has produced several interesting results. For example:\n",
    "- Identifying [authorship in Shakespeare's plays](http://elizabethan-theatre.org/wps/wp-content/uploads/2015/07/Double-Falsehood-by-Ryan-Boyd-Psychological-Science-2015.pdf)\n",
    "- Longitudinal studies of Mayor [Giuliani's adaptation to the crisis of 9/11](https://www.sciencedirect.com/science/article/abs/pii/S0092656602923494)\n",
    "- Detecting people's [personality from their writing style](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2885844/?_escaped_fragment_=po=2.77778)\n",
    "- Monitoring [public anxiety](https://www.tandfonline.com/doi/abs/10.1080/10410236.2011.571759) and public mood (which is [associated with stock prices](https://arxiv.org/pdf/1010.3003&))\n",
    "\n",
    "But, all these studies use fairly basic word count techniques, or, slightly more advanced sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Post-2013 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "Word vectors, or 'word embeddings', developed in the late 1990s by Yoshua Bengio, Geoffrey Hinton, et al. But, their significance was only appreciated in about 2013.\n",
    "\n",
    "Before about 2013, most computational analysis of words focused on the textual appearance of a word; for example, 'dog' was similar to 'doggy' (same stem), similar to 'god' (same letters) and 'hog' (2 letters in the same order), but without any similarity to 'cat' (no similar letters). There seemed to be no way to make computers understand that 'dog' and 'cat' are both family pets and thus semantically related.\n",
    "\n",
    "The breakthrough came from Wittgenstein's (1953; also Frith, 1957) insight: the meaning of words comes from the context in which they are used. Consider the sentence 'the X sat on the mat' - most people would agree that 'dog' and 'cat' are plausible candidates for X. That is to say, 'dog' and 'cat' occupy similar roles within similar sentences. \n",
    "\n",
    "Word embeddings encode the meaning of words not in terms of the words themselves, but, in terms of the context in which the words appear. By analyzing billions of webpages and books, statistical models are built based on which words occur in the same context. These models _seem_ to encode meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google and other companies are using word embeddings to [map the meaning of all words](https://projector.tensorflow.org/) - created by analysing the sentence context in which words are used accross _all_ books, webpages, news sources and other archives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find words used in similar contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.019249Z",
     "start_time": "2020-04-18T19:49:20.324Z"
    }
   },
   "outputs": [],
   "source": [
    "def getTokensNearVector(nlp, vector, limit=300, debug=False):\n",
    "    '''\n",
    "    takes a vector (does not need to be based on a word)\n",
    "    returns a list of the nearest tokens\n",
    "    all returned vectors are lower case, so duplicates are removed\n",
    "    '''\n",
    "        \n",
    "    # INPUT CHECKS\n",
    "    # if input is vector\n",
    "    if isinstance(vector, np.ndarray):\n",
    "        input_is_string=False\n",
    "        # Format the input vector for use in the distance function\n",
    "        p = [np.array(vector)]\n",
    "        if debug==True:\n",
    "            print('INPUT IS A VECTOR')\n",
    "            print('type(p):', type(p))\n",
    "        \n",
    "    # if input is string\n",
    "    elif isinstance(vector, str):\n",
    "        input_is_string=True\n",
    "        #print(f'the input was a string:  \"{vector}\"')\n",
    "        #print('converting each token into a vector and taking the average\\n')\n",
    "        doc = nlp(vector)\n",
    "        vectors = []\n",
    "        for tok in [tok for tok in doc if tok.is_alpha]:\n",
    "            try:\n",
    "                vectors.append(tok.vector)\n",
    "            except:\n",
    "                print('error getting vector for ', tok.text)\n",
    "        #sum vectors\n",
    "        vector_sum=0\n",
    "        for v in vectors:\n",
    "            vector_sum=vector_sum+v\n",
    "        \n",
    "        #get mean vector\n",
    "        vector=vector_sum/len(vectors)\n",
    "        # Format the input vector for use in the distance function\n",
    "        p = [np.array(vector)]\n",
    "\n",
    "        if debug==True:\n",
    "            print('INPUT IS A STRING')\n",
    "            print('type(p):', type(p))\n",
    "            #print('THIS IS THE CONVERTED VECTOR: ', vector)\n",
    "            \n",
    "    elif isinstance(vector, list):\n",
    "        print('THIS IS A LIST - TAKING FIRST ITEM')\n",
    "        vector=vector[0]\n",
    "    else:\n",
    "        print('UNKNOWN TYPE: ', type(vector))\n",
    "        \n",
    "    # Imports\n",
    "   \n",
    "    #import spacy\n",
    "    #nlp = spacy.load(\"en_core_web_lg\")\n",
    "    # https://stackoverflow.com/questions/54717449/mapping-word-vector-to-the-most-similar-closest-word-using-spacy\n",
    "    \n",
    "    # Format the vocabulary for use in the distance function\n",
    "    ids = [x for x in nlp.vocab.vectors.keys()]\n",
    "    vectors1 = [nlp.vocab.vectors[x] for x in ids]\n",
    "    vectors2 = np.array(vectors1)\n",
    "        \n",
    "    if debug==True:\n",
    "        print('THIS IS THE FORMATTED VECTOR (p) BEING SENT TO THE DISTANCE FUNCTION')\n",
    "        print(p)\n",
    "        print('THESE ARE THE IDS')\n",
    "        print(ids[:100])\n",
    "        print('THESE ARE vectors1 - BEFORE BEING FORMATTED')\n",
    "        print(vectors1[:2])\n",
    "        print('THESE ARE vectors2 - ALL NLP VECTORS FORMATTED FOR COMPARISON BY DIST FUNCTION')\n",
    "        print(vectors2[:2])\n",
    "    \n",
    "    # measure distances\n",
    "    dist = distance.cdist(p,vectors2)[0]\n",
    "    \n",
    "    # create df of results\n",
    "    df = pd.DataFrame({'distance':[x for x in dist]})\n",
    "    df.sort_values('distance', inplace=True)\n",
    "    \n",
    "    if debug==True:\n",
    "        print('THIS IS THE DATAFRAME OF DISTANCES')\n",
    "        display(df)\n",
    "    \n",
    "    # print results\n",
    "    words = []\n",
    "    for i in df.index[:limit]:\n",
    "        word_id = ids[i]\n",
    "        output_word = nlp.vocab[word_id].text\n",
    "        output_word = output_word.lower()\n",
    "        if output_word not in words:\n",
    "            words.append(output_word.lower())\n",
    "            \n",
    "    #if input was string, then remove input tokens\n",
    "    if input_is_string==True:\n",
    "        words = [word for word in words if word not in [tok.lower_ for tok in doc]]\n",
    "            \n",
    "    print('\\nTOKENS NEAR VECTOR:   ', ' '.join(words), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try single words, such as: 'blue', 'summer', 'university' or 'psychology'. <br>\n",
    "Also, try combinations of words: 'sad happy' (yields 'hope'), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.020250Z",
     "start_time": "2020-04-18T19:49:20.327Z"
    }
   },
   "outputs": [],
   "source": [
    "getTokensNearVector(nlp, 'happy angry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Comparing word meaning?\n",
    "Most people would agree that 'dog' is more similar to 'cat' than 'road' - and the models agree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.021250Z",
     "start_time": "2020-04-18T19:49:20.330Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "words = ['dog', 'cat', 'car', 'road', 'traffic', 'ideas']\n",
    "    \n",
    "compareMeanings(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Comparing sentence meaning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "We can also use these models to compare sentences. Consider the following (all using unique words). Just by averaging the word embeddings for each sentence, we can see, that the model corresponds broadly to our human judgement. <br>\n",
    "<br>\n",
    "Change the sentences and re-run the cell to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.021250Z",
     "start_time": "2020-04-18T19:49:20.332Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "sentences = ['the pilot flew the plane', \n",
    "             'trees do not grow on rocks',\n",
    "             'i broke my arm', \n",
    "             'he snapped his leg',\n",
    "             'a helicopter soared up',\n",
    "             'plants need lots of water']\n",
    "\n",
    "compareMeanings(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring sentences against target meaning\n",
    "The idea here comes from the [Garten et al. (2018)](https://link.springer.com/article/10.3758/s13428-017-0875-9) who argue that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.022250Z",
     "start_time": "2020-04-18T19:49:20.334Z"
    }
   },
   "outputs": [],
   "source": [
    "seed_terms = 'died'\n",
    "\n",
    "sentences = [\"Superb excellent ++\",\n",
    "             'accessible car park',\n",
    "             'my arm broke', \n",
    "             'she was dying', \n",
    "             'he died later',\n",
    "             'died died died']\n",
    "\n",
    "for sent in sentences:\n",
    "    score = round(nlp(sent).similarity(nlp(seed_terms)), 2)\n",
    "    print(\"Similarity to 'died':  {:<4}  '{:}'\".format(score, sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Adding and subtracting meanings?\n",
    "If words are represented as numbers, then, can we do mathematics with the meanings?\n",
    "\n",
    "What if we start with the meaning of 'king' subtract 'man' and add 'woman' - what would the outcome be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.023250Z",
     "start_time": "2020-04-18T19:49:20.336Z"
    }
   },
   "outputs": [],
   "source": [
    "def mathsOnMeaning(start, subtract, add):\n",
    "    x = nlp.vocab[start].vector - nlp.vocab[subtract].vector + nlp.vocab[add].vector\n",
    "    \n",
    "    similar_words = []\n",
    "    for word in nlp.vocab:\n",
    "        if not (word.has_vector & word.is_lower):\n",
    "            continue\n",
    "        if not ((word.text!=start)&(word.text!=add)&(word.text!=subtract)):\n",
    "            continue\n",
    "     \n",
    "        similarity = cosine_similarity(x, word.vector)\n",
    "        #similarity = word.similarity(x)\n",
    "        similar_words.append((word, similarity))\n",
    "        \n",
    "    similar_words = sorted(similar_words, key=lambda item: -item[1])\n",
    "    result = [w[0].text for w in similar_words[:5]][0]\n",
    "    print(f\"'{start}' minus '{subtract}' add '{add}' = {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the words near 'died dead and dying'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.023250Z",
     "start_time": "2020-04-18T19:49:20.338Z"
    }
   },
   "outputs": [],
   "source": [
    "v = nlp('died').vector\n",
    "getTokensNearVector(nlp, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, people tend to die in two ways: unintentional and intentional (murder). <br>\n",
    "To find words associated with intentionall death, we subtract the concept of 'accident' and add the concept 'murder'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.024250Z",
     "start_time": "2020-04-18T19:49:20.340Z"
    }
   },
   "outputs": [],
   "source": [
    "v = nlp('died').vector - nlp('accident').vector + nlp('murder').vector\n",
    "getTokensNearVector(nlp, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can look at the words associated with 'died' excluding the 'murder' and focusing on 'accidents'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.025250Z",
     "start_time": "2020-04-18T19:49:20.342Z"
    }
   },
   "outputs": [],
   "source": [
    "v = nlp('died').vector + nlp('accident').vector - nlp('murder').vector\n",
    "getTokensNearVector(nlp, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.025250Z",
     "start_time": "2020-04-18T19:49:20.343Z"
    }
   },
   "outputs": [],
   "source": [
    "v = nlp('rome').vector + nlp('uk england').vector \n",
    "getTokensNearVector(nlp, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.026250Z",
     "start_time": "2020-04-18T19:49:20.345Z"
    }
   },
   "outputs": [],
   "source": [
    "v = nlp('austria').vector\n",
    "getTokensNearVector(nlp, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.027250Z",
     "start_time": "2020-04-18T19:49:20.349Z"
    }
   },
   "outputs": [],
   "source": [
    "start = 'he'\n",
    "subtract = 'doctor'\n",
    "add = 'woman'\n",
    "\n",
    "start = 'doctor'\n",
    "subtract = 'man'\n",
    "add = 'woman'\n",
    "\n",
    "\n",
    "v = nlp.vocab[start].vector - nlp.vocab[subtract].vector + nlp.vocab[add].vector\n",
    "#print(v)\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "# Format the vocabulary for use in the distance function\n",
    "ids = [x for x in nlp.vocab.vectors.keys()]\n",
    "vectors1 = [nlp.vocab.vectors[x] for x in ids]\n",
    "vectors2 = np.array(vectors1)\n",
    "#print(vectors2)\n",
    "        \n",
    "# measure distances\n",
    "dist = distance.cdist([np.array(v)],vectors2)[0]\n",
    "    \n",
    "# create df of results\n",
    "df = pd.DataFrame({'distance':[x for x in dist]})\n",
    "df.sort_values('distance', inplace=True)\n",
    "\n",
    "# print output\n",
    "words = []\n",
    "for i in df.index[:50]:\n",
    "    word_id = ids[i]\n",
    "    output_word = nlp.vocab[word_id].text\n",
    "    output_word = output_word.lower()\n",
    "    if output_word not in words:\n",
    "        words.append(output_word.lower())\n",
    "        \n",
    "words         \n",
    "#remove input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.027250Z",
     "start_time": "2020-04-18T19:49:20.350Z"
    }
   },
   "outputs": [],
   "source": [
    "getTokensNearVector(nlp, 'rome pizza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.028250Z",
     "start_time": "2020-04-18T19:49:20.352Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "cosine_similarity = lambda x, y: 1-spatial.distance.cosine(x, y)\n",
    "print(\"apple vs banana: \", cosine_similarity(nlp.vocab['apple'].vector, nlp.vocab['banana'].vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.029251Z",
     "start_time": "2020-04-18T19:49:20.353Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "start = 'king'\n",
    "subtract = 'man'\n",
    "add = 'woman'\n",
    "\n",
    "mathsOnMeaning(start, subtract, add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try: 'uk' minus 'london' add 'geneva'\n",
    "- Try: 'berlin' minus 'germany' add 'france'\n",
    "- Try: 'india' minus 'curry' add 'pizza'\n",
    "- Try: 'man' minus 'boy' add 'girl'\n",
    "- Try: 'paris' minus 'france' add 'uk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## 'Thought vectors'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Geoffrey Hinton, one of the developers of 'deep learning' techniques, talks about the above examples that focus on single words as only the first step: he (and Google) are working towards 'thought vectors'. \n",
    "\n",
    "While the above examples represent the meanings of words in a 300 dimensional mathematical space, they aim to map thoughts, ideas, sentences, and even whole articles and books into a multi-million dimensional space.\n",
    "\n",
    "Transcritpts, texts, and even audio-visual data will become open ho high-level semantic analysis (meaning clusters, emotion, contradictions, meta-perspectives and dialogical tensions).\n",
    "\n",
    "- Longitudinal analyses of an individual life-course\n",
    "- Comparison of perspectives of groups in conflict\n",
    "- Identifying the emergence of novelty and new ideas\n",
    "\n",
    "And, all done, on any size of dataset, in real-time, and for almost zero cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Hospital Staff Responding to Criticism \n",
    "Accidents in hospitals are a leading cause of death. At least 10% of people going into hospital come out with a new health problem. Causes include: delays, hospital infections, errors, wrong-site surgery, misdiagnosis, medication errors, not reading patient notes etc. \n",
    "\n",
    "While aviation, construction, and heavy industry have all become much safer over the last 50 years, healthcare has remained dangerous, and seems resistant to improvement. Some hospitals have a 'blame culture' with high defensiveness.\n",
    "\n",
    "One idea is that medical staff could learn from patients: patients know their own bodies, are usually the only person who has been at all the meetings, and have increasing access to medical knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Research context\n",
    "Question: How do healthcare staff respond to critical feedback?\n",
    "    \n",
    "Data: Online discussions between patients and staff in the UK (about 250k paired dialogs)\n",
    "    \n",
    "- What is the divergence of perspective between patients and staff?\n",
    "\n",
    "- How are staff listening to and/or ignoring the patient point of view?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.030250Z",
     "start_time": "2020-04-18T19:49:20.356Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# loading the data\n",
    "df = pd.read_csv('co_crit3.csv')\n",
    "df['text_doc'] = df['post_body'].apply(nlp)\n",
    "df['post_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Comparing perspectives: words\n",
    "The plot below compares the text of 'patient stories' with 'staff responses' in terms of past-tense verbs.\n",
    "- The vertical axis is for patients; horizontal axis is for staff\n",
    "- Words in the top-right are common to both patients and staff (i.e., was, were, did, had)\n",
    "- Words in the top-left are peculiar to patients (i.e., took, died, called, started, tried, refused)\n",
    "- Words in the bottom-right are peculiar to staff (i.e., raised, expectations, experiences, mentioned, expressed)\n",
    "- Clicking on a word 'died' shows the original text (85 patient posts, 1 staff post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.030250Z",
     "start_time": "2020-04-18T19:49:20.357Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "#html = scattertextTag('VBD')\n",
    "file_name = 'co_crit3_past.html'\n",
    "#open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width=1300, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Comparing perspectives: themes\n",
    "Based on an examination of the patient and staff text, we can formulate two ideas.\n",
    "\n",
    "1. Patients seem to be talking about 'real' clinical issues.\n",
    "\n",
    "2. Staff seem reluctant to address these clinical issues, and prefer to talk about 'valuing feedback' and 'patient experience'\n",
    "\n",
    "To test these ideas, we can make some 'topic themes' that we then map into the semantic space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.031250Z",
     "start_time": "2020-04-18T19:49:20.359Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "themes = {\n",
    "    'clinical': ['clinical', 'unsafe', 'doctor', 'nurse', 'operation', 'diagnosis', 'misdiagnosis'],\n",
    "    'pain': ['agony', 'suffering', 'pains', 'suffer', 'screaming'],\n",
    "    'feedback': ['value', 'feedback', 'learning', 'listen', 'listening'],\n",
    "    'experience': ['experience', 'concern', 'concerns', 'feelings']\n",
    "}\n",
    "\n",
    "html = scattertextThemes(themes)\n",
    "file_name = 'co_crit3_themes.html'\n",
    "open(file_name, 'wb').write(html.encode('utf-8'))\n",
    "IFrame(src=file_name, width=1000, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Which responses have least perspective taking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Using the 'word embeddings' method, can we identify the staff response with the least perspective taking?\n",
    "\n",
    "Method: compare the word vectors for the staff responses with the original patient story and find the biggest difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.032250Z",
     "start_time": "2020-04-18T19:49:20.361Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "df_index = 1\n",
    "df = df.sort_values(by='similarityToOrigin', ascending=True, axis='index').reset_index(drop=True)\n",
    "selected_row = df.iloc[df_index, :,]\n",
    "print(f\"ORIGINAL POST\\n{selected_row[['thread_originText']][0]}\")\n",
    "print(f\"STAFF RESPONSE\\n{selected_row[['post_body']][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "The distressed cancer patient had her glads that produce calcilum damaged during surgery resulting in paralysis and provided conflicting information about a blood clot in her brain. \n",
    "\n",
    "Despite the clinical error, her main concern is the rudeness of staff who have ignored her feelings.\n",
    "\n",
    "The staff response is generic: 'Thank you for your feedback'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Which responses have most psychological distancing?\n",
    "Psychological distancing (not distanciation) refers to defensive routines that hold problematic perspectives at a semantic distance, so as to neutralise their transformative potential. \n",
    "\n",
    "One type of psychological distancing is to refer to problematic perspectives as 'beliefs' or 'experiences'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T19:49:34.032250Z",
     "start_time": "2020-04-18T19:49:20.363Z"
    },
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "sentence_subject = ['you']\n",
    "sentence_object = ['experience']\n",
    "sentence_contains = ['impression', 'subjective']\n",
    "\n",
    "findSentence(sentence_subject, sentence_object, sentence_contains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "In the above excerpt has two types of psychological distancing:\n",
    "\n",
    "First: The distressed patient is writing about a formal complaint they have submitted to the hospital, which has been ignored, and which they cannot get any feedback on (they have tried ringing and calling). Accordingly, the patient has resorted to a public post. Despite multiple failings by the hospital (known to the hospital) the problem is described in a psychologising manner:\n",
    "- 'the experience'\n",
    "- 'you would feel'\n",
    "- 'your concerns'\n",
    "- 'this impression'\n",
    "\n",
    "Second: The staff (as a 'we') psychologises themselves, thus distancing from implications for action\n",
    "- 'we understand'\n",
    "- 'we are sorry'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "My substantive interest is to understand the ways that people don't listen. \n",
    "- distancing (psychologising problems)\n",
    "- denial (ignoring topics)\n",
    "- denigration (undermining the motive, stigma, expertise)\n",
    "- rationalisation (individualising problems as one-off problem)\n",
    "\n",
    "These strategies of not-listening guard the threshold between self and other; creating psychological comfort at the cost of transformative dialogue. \n",
    "\n",
    "If we want to understand how 'the social' leads to change, we need to understand 'semantic contact' - how alternative perspectives are accepted, rejected or modified.\n",
    "\n",
    "But, for the purposes of our discussion, I also want to raise methodological questions:\n",
    "- Is there a meeting between sociocultural psychology and NLP?\n",
    "- What can NLP do for sociocultural psychology? - extra evidence, news tools, scale-up analyses?\n",
    "- What can sociocultural psychology do for NLP? - conceptualizing the psychology of 'thought vectors', understanding the inherent dialogicality of language?\n"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "345px",
    "width": "256px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
